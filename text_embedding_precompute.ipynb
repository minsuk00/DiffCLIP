{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embeddings pre-compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/choi/Downloads/miniconda3/envs/diff-clip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPVisionModelWithProjection\n",
    "\n",
    "import json\n",
    "import os\n",
    "from IN_id_to_classname import IMAGENET2012_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_path = \"/scratch/choi/model/stable-diffusion-v1-5\"\n",
    "text_encoder = CLIPTextModel.from_pretrained(sd_path, subfolder=\"text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(sd_path, subfolder=\"tokenizer\")\n",
    "# tokenizer2 = CLIPTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "# text_encoder = CLIPTextModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "clip_model = CLIPModel.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "# clip_vision_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "#     \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.load(\n",
    "#     open(\"/scratch/choi/dataset/ImageNet100/_img_text_pair_train.json\")\n",
    "# )  # list of dict: [{\"image_file\": \"1.png\", \"text\": \"A dog\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "label_list = os.listdir(\"/scratch/choi/dataset/ImageNet100/train\")\n",
    "for label in label_list:\n",
    "    text_list.append(\"a photo of \" + IMAGENET2012_CLASSES[label])\n",
    "label_list.append(\"\")\n",
    "text_list.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = tokenizer(text_list, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 77])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP-L (SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = text_encoder(text_tokens)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 77, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict(zip(label_list, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"n02114855\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a, \"IN100_text_embedding_dict_L.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP-H (with projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out2 = clip_model.text_projection(clip_model.text_model(text_tokens)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dict(zip(label_list, out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[\"n02114855\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(b, \"IN100_text_embedding_dict_with_projection_H.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from IN_id_to_classname import IMAGENET2012_CLASSES\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        # RandomResizedCrop(224, interpolation=3),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(256, interpolation=3),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_val = datasets.ImageFolder(\"/scratch/choi/dataset/ImageNet100/val\", transform=transform_train)\n",
    "classnames = [IMAGENET2012_CLASSES[id] for id in dataset_val.classes]\n",
    "prefix = \"a photo of a \"\n",
    "text_inputs = [prefix + name for name in classnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "model = CLIPTextModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
    "\n",
    "inputs = tokenizer(text_inputs, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "text_embeds = outputs.text_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(text_embeds, \"IN100_classnames_text_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"IN100_classnames_text_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = a\n",
    "text_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"IN100_text_embedding_dict_L.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"n02109047\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],\n",
       "        [-0.3711, -1.4497, -0.3401,  ...,  0.9489,  0.1867, -1.1034],\n",
       "        [-0.5107, -1.4629, -0.2926,  ...,  1.0419,  0.0701, -1.0284],\n",
       "        ...,\n",
       "        [ 0.5006, -0.9552, -0.6610,  ...,  1.6013, -1.0622, -0.2191],\n",
       "        [ 0.4988, -0.9451, -0.6656,  ...,  1.6467, -1.0858, -0.2088],\n",
       "        [ 0.4923, -0.8124, -0.4912,  ...,  1.6108, -1.0174, -0.2484]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff-rep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
